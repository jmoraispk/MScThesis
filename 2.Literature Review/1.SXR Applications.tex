\section{Social XR Applications}
\label{sec:sxr_applications}

As mentioned, the service whose provision we aim to optimise is social XR conferences. More specifically, first and foremost, we need to find about the application requirements in terms of throughput and latency. Then, it is required a model for the \ac{UL} and \ac{DL} traffic, perhaps a random packet arrival distribution, as well as a survey on the architecture of XR conference systems to understand how \ac{UL} and \ac{DL} traffic relate. Information on where the transmitters should be placed is key also, i.e. what are and where are the \acp{UE}.

Subsequently, we need a model for user movement in the meeting room since user mobility influences the propagation conditions and radio channel variability, which are important factors to account for during the radio resource management procedure. And finally, it is useful to review the best attempts to optimise the \ac{QoS} provision for XR applications over wireless.

Let us start by clarifying the term \ac{XR}. By definition, it refers to all real-and-virtual combined environments and human-machine interactions generated by computer technology and wearables. In essence, it includes \ac{AR}, \ac{VR} and everything in between. Movies such as ``Ready Player One'' \cite{readyPlayerOne} anticipate what can be the future of these technologies; see Figure \ref{fig:kingsman_ar} for a snapshot of an AR meeting use-case taken from a movie.

\image{Literature Review/kingsmanv2.png}{\ac{AR} meeting snapshot from movie `Kingsman: The secret service' \cite{kingsman}}{fig:kingsman_ar}{.46}


Regarding the state of the art in XR applications optimisation from the wireless perspective, the topic has not been very active. Although there is no lack of reasons to research how to remove the wire has that connects the \ac{HMD} to the source of content, such as improving the immersiveness of experience and reducing the risk of tripping hazards, besides all reasons to improve \ac{AR}/\ac{VR} systems, it seems there has not been much attention from the wireless communications research community. The vast majority of well-cited papers dates no later than 2017.

\subsection*{Requirements}

Fortunately, with respect to requirements there has been plenty of speculation. Elbamby \ii{et al.} \cite{Elbamby_towards_low_latency_vr} does a back-of-the-envelope calculation considering each human eye is able to  see up to 64 million pixels (having 150\tdeg horizontal and 120\tdeg vertical \ac{FOV}, and resolution of 60 pixels per degree), at 120 \ac{FPS} (argued required to generate a real-like view), up to 15.5 billion pixels per second are needed requiring 1 Gbps (gigabit per second) speeds to transmit if each coloured pixel is stored with a high resolution of 36 bits (12 bits per colour/component) and the stream is compressed by 600 times with a state of the art H.265 encoder. Other authors \cite{bastug_towards_interconnected_vr} reach even higher numbers.

Of course, as a back of the envelope calculation it has its utility, but some important factors are missing. Namely, there exist many application-layer techniques to reduce the required bitrate, e.g. frame prediction in applications where 360\tdeg video is involved \cite{frame_prediction}. Also, 64 million pixels per eye equates to almost double the pixel count of an 8K screen (7680 by 4320 pixels). Therefore these ideal conditions are far from realistic in the near future. The current best display achieves 1700 by 1440 pixels per eye, which is less than 2K, with a refresh rate of 90 Hz \cite{vive_cosmos_elite}. To achieve higher numbers is currently required  pixel interpolation \cite{pimax}.

On the other hand, the throughput estimation can be higher. \ac{HEVC}, or H.265 for short, is a complex set of algorithms that, as the name suggest, aims to optimise coding efficiency, i.e. to reduce the size stream data as much as possible while keeping the quality imperceptible unchanged. These complex algorithms take many tenths or even hundreds of milliseconds to encode, making them less suitable for real-time transmissions. Within the standard there are options that achieve lower encoding and decoding latencies, but at expense of compression ratio.

In summary, the application resolutions and frame rates matter considerably, and it is yet unknown what would constitute good \ac{QoE}. Since \cite{Elbamby_towards_low_latency_vr} and \cite{vr_on_the_edge} agree an entry-level VR would need around 100 Mbps, and considering 3GPP \cite{3GPP_xr} 50-100 Mbps estimate (for the most common streaming strategy) and \cite{bastug_towards_interconnected_vr} proposal of 100-200 Mbps, we may settle for 100 Mbps of throughput requirements for the near-future VR experience, in order to establish a concrete first target for near-future VR.

To finalise the required throughputs requirements a streaming strategy needs to be defined and 3GPP \cite{3GPP_xr} defines mainly two, viewport-independent (most common) and viewport-dependent streaming. The difference lies preparing the scene to send to a user independently of the user thus sending the full XR scene, or based on what the user is looking at, respectively. When the user point of view is considered, only the data that user requires can be sent, which should allow for a reduction in the required bitrate by a factor of two to four compared to sending the full XR scene. 


Regarding latencies, the distinction between two latencies should be made. There is the \ac{RTT} latency and there is the \ac{MTP}. The latter is basically an headset requirement: from the time a movement occurs to the changes be reflected in the display there must not go more than 20 ms \cite{3GPP_xr, Elbamby_towards_low_latency_vr}. The first, on the other hand, is on the order of several tenths or hundreds of milliseconds and is the time it takes for the information to go from the user to the main XR server and an answer to come back to the user. This is the latency we are interested in and consists of several components, as shown in Figure .%ref to figure

\todo[inline]{include figure for several delays in the end-to-end chain}

Two-way delay contributions include, sensor sampling, encoding, one-way network delay (router and access point processing delays, queuing delays, transmission delays and propagation delays), decoding, image processing algorithms on the cloud, encoding once again, another one-way network delay, decoding, local frame rendering and display refresh delay. Most of these delays are practically imperceptible compared to others. For instance, the delay from sensor sampling is less than 1 ms while the display delay tends to be 10-15 ms \cite{bastug_towards_interconnected_vr} although it is expected to drop to less than 5 ms \cite{vr_on_the_edge}. 

Additionally, currently just the computation delay alone exceeds 100 ms \cite{vr_on_the_edge}. However, by making a smart use of caching, bringing the processing power closer to the access (edge computing) and improving of communications \cite{Elbamby_towards_low_latency_vr, bastug_towards_interconnected_vr, 3GPP_xr} this delay is foreseen to drop to below 10 ms \cite{vr_on_the_edge}. 

The target for \ac{RTT} of 50 ms is given by \cite{3GPP_xr}. \cite{vr_on_the_edge} says 30 ms is a better value. Also, 3GPP defines in \cite{3GPP_xr} a \ac{5GI} of value 82 for \ac{AR} applications mapping to \ac{QoS} characteristics of 10 ms radio interface latency and $10^{-6}$ \ac{PER}. Thus, we conclude that 10ms is an hard upper limit of accepted latency, and that the preference is to as low as possible.

\subsection*{Architecture and Capture System}

The standardisation by 3GPP \cite{3GPP_xr} is in agreement with \cite{multi_sensor_tno} relatively to architecture of the network. Remarks relevant to our study lie in terms of traffic patterns and relationships between uplink and downlink. Figure \ref{fig:xr_setting} shows the simplified capture system and relevant architecture details. 

\image{Literature Review/xr_setting.png}{Simplified architecture and capture system for XR conference}{fig:xr_setting}{.46}

There is a single uplink stream per user for pre-transmission stream synchronisation. Therefore, the information captured from both cameras is aggregated into one single stream before transmitted, which facilitates the synchronisation in the cloud. This aggregation should happen as close to the source as possible to avoid having the destination wait to receive both recordings of the same user with the same timestamps. Such loss of synchrony can happen when each camera sends its information separately due to the delays of different paths in the network.

Furthermore, at this stage, the information recorded constitutes the user representations. Although aggregation and processing can make a user representation more compact, this is infeasible in the near future due to latency constraints. Therefore, each participant receives the information of each other participant, which is the information recorded by two (or more) capture devices, in case of viewport-independent streaming. With viewport-dependent streaming the users receive only the users they are looking at. Also, in case of an \ac{AR} meeting, the users only receive information on the remote (non-physically present) users, since the physically present users can be seen through the AR glasses. 

With respect to how the capture should be made, \cite{double_rgb_tno} tests and proposes a dual-camera setup where each camera can record \ac{RGB} and depth information. The cameras should be placed at head height, roughly 30\tdeg from the normal to the user. Exactly how information is stored and transmitted, being double RGBD (RGB plus depth) or 3D mesh or as a point cloud, is yet to be determined, despite playing an important role in the throughput and latency requirements \cite{3GPP_xr}.


\subsection*{Model for Human Head Movement and Traffic}

To the best of our knowledge, there are no human head movement models in literature that suit a conference. Some authors have recorded head and eye movement for different scenarios of 360\tdeg video \cite{avtrack360, dataset2}. However, not only does 360\tdeg video have a considerably different dynamic than a real-time conference, but none of the videos were remotely close to a meeting room.

Likewise, we did not find traces in literature of XR conferences traffic models. Traffic models on its own are relatively hard to find, and it has proven to be an impossible task for such a new application. After all, there still is no agreement \cite{3GPP_xr, multi_sensor_tno} in many important details of the application


\subsection*{Difference to other approaches to optimisation}

In \cite{8395443} is presented a framework that analyses the performance of VR services over wireless networks. The framework captures the tracking accuracy, transmission delay, and processing delay, but most radio characteristics such as frequency-selective fading, antenna configurations and blockage effects are not considered. The authors of \cite{cutting_the_cord} study the impact of blockage by hand, head and body on wireless \ac{mmWave} links, and suggest an algorithm to overcome the corresponding challenges. The proposed solution uses a fixed relay to increase robustness against blocking and is assessed in an experimental setup. The attainable gains strongly depend on numerous assumptions and deployment configurations which are not described in any detail. 


Other more common approach to the problem is to focus on the network perspective \cite{7997740, 8319985}, however parameters on the radio interface are often barely considered. Hence, we may conclude there is a clear lack of research on physical layer optimisation targetting virtual reality applications' \ac{QoS} requirements. 