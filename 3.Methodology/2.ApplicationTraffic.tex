\section{Application Traffic Model}
\label{sec:at}

The model that determines the packets arrival times is coded in Python.
In essence, this generates traffic resembling video streaming. We chose video streaming to shape our incoming traffic. Today, it still is too soon to tell what kind of format will be used to represent a person in 3 dimensions. From Point Clouds to Mesh formats, there's a plethora of formats that need to be studied in light of throughput requirements, capture and encoding speeds, ease of stitching multiple streams together, etc. Frame-based (happens in video streaming) is a possibility and it certainly serves our purposes. Moreover, as we'll see further down this section, we can tune some traffic characteristics, e.g. burstiness, to test and access this impact in several scenarios.

In modern video streaming, not all frames are made equal. There are I frames (Intra-coded frames), that constitute the normal and complete frame, and then there are frames that are much smaller: P frames (previous dependent), and B frames (bi-dependent, previous and forward dependent). P and B frames can be considerably smaller than the I frame because they can be derived from adjacent frames, therefore don't need to carry that much information \cite{wiki:video_frames}. A simplistic way of pondering about they, is by thinking they carry the differences only, while the I frame carries the complete image.


The first phase of the traffic model is the computing the size of the I frame ($I_{size}$). Then, the size of the P frame is directly determined by the ratio between I and P frames ($IP_{ratio}$). Having the frame sizes, and time interval between frames (equal to the inverse of the frame rate), given by the inverse of the number of \ac{FPS}, we can tell when certain frames are generated. Each frame is then distributed across many packets, the bigger the frame, the more the required packets. Finally, the packets are spread out, depending on the burstiness and overlap parameters, such that it matches the arrival rates of the packets to the uplink buffers (camera buffers), and the downlink buffers (\acs{BS}(s) buffers, to be sent to the user's headset).


Firstly, we have two ways of computing the I frame size. Each camera will generate frames that are distributed in packets. The packets are then injected in the uplink:
\begin{itemize}
    \item From the average throughput - Using the average required throughput and computing frame sizes directly. It allows us to abstract the process from most application layer parameters. Equation \eqref{eq:avg_bitrate} shows how to derive the I frame size from the average bitrate of a video stream:
    \begin{equation} \label{eq:avg_bitrate} 
        I_{size} = \frac{R_{avg_UL}}{FPS \cdot GoP \cdot \left( 1 + (GoP - 1) IP_{ratio}\right)}
    \end{equation}
    
    \item From the application parameters - Requires many application layer parameters: camera resolution (bits per frame), pixel format (channels per pixel), pixel depth(bits per channel), depth resolution ( bits per channel) (note that the cameras are RGB-D cameras, also record depth information), compression ratio, and throughput reduction techniques like tile prediction.
    All this is taken into account in equation \eqref{eq:app_layer_param1}. 

    \begin{align}
        I_{{\text{size}}_\text{uncomp.}} = \ & \text{camera resolution} \times \nonumber \\
        & \times \left[  \text{pixel format} \cdot \text{pixel depth} + 1 \cdot \text{depth resolution} \right]  \label{eq:app_layer_param1}
    \end{align}

    And the size of the I frame, pos-compression, is given by Equation \eqref{eq:app_layer_param2}. In this equation, RCR stands for the Remaining Compression Ratio, and symbolises the compression that is left after taking into account the reduction from using P frames. This needs to be done because the video encoding algorithm creates this frames also, therefore the amount of compression given by total video compression has a component of IP frame creation and some remaining compression component, which is the compression we want to apply to the I frame since the IP component we are naturally already applying by considering I and P frames.
    \begin{gather}
        I_{size} = I_{{\text{size}}_\text{uncomp.}} \times \text{RCR}  \label{eq:app_layer_param2} \\
        \text{RCR} = \frac{\text{Total Compression Ratio}}{\text{IP Compression Ratio}} \label{eq:app_layer_param3} 
    \end{gather}
    \begin{align}
        \text{IP Compression Ratio} & = \ \frac{\text{Avg. Frame Size after IP compression}}{\text{Avg. Frame Size before IP compression}} \nonumber \\
        & = \ \frac{I_\text{size}}{I_\text{size} \frac{1 + \text{IP}_\text{ratio} \cdot (GOP - 1)}{GOP}} = \frac{GOP}{1 + \text{IP}_\text{ratio} \cdot (GOP - 1)}\label{eq:app_layer_param4} 
    \end{align}

    Therefore, bringing all together to a final equation:
    \begin{equation}
        I_{size} = \frac{I_{{\text{size}}_\text{uncomp.}} \times \text{Total Compression Ratio} \times GOP}{1 + \text{IP}_\text{ratio} \cdot (GOP - 1)}
    \end{equation}

    \bb{Note}: a better nomenclature would be $\text{CR}_\text{T}$, $\text{CR}_\text{R}$, $\text{CR}_\text{IP}$, respectively for the total, remaining and IP-attributed Compression Ratios.

\end{itemize}



So far, we've considered only P frames, but just as easily B frames can be added with a I to B or P to B frame ratio. Since this is kind of application has very high latency requirements, small GoP sizes are favoured, because more often a full refresh is done, increasing the reliability. Because of the small GoP size, including B frames would not yield any significant effect. 

The main parameters are: \acs{GoP} size of 6, 30 \acs{FPS}, a ratio of 20\% between P frames and I frames. Furthermore, the size of P frames and the size of I frames is constant, and depends only on the average throughput.

All packets are considered full, containing 1500 bytes. In reality, this case for basically all packets.

In the uplink, it's assumed all packets from a certain frame are instantaneously available. We make this modelling choice because the packets are definitely coming into the buffers much faster than they'll be dispatched. Downlink requires careful approach because the packets have come from the Multi-point Control Unit (MCU) and passes through many nodes in the Internet, which due to throughput bottlenecks and buffering, may arrive more spaced. To mimic this effect we use a burstiness parameter. See figure \ref{fig:burst}.

\threeimagessidebyside{Implementation/AppLayer/burst-1.png}{Burstiness = 1}{fig:packet1}{.325}{Implementation/AppLayer/burst-2.png}{Burstiness = 0.7}{fig:packet2}{.33}{Implementation/AppLayer/burst-3.png}{Burstiness = 0.3}{fig:packet3}{.33}{full caption here}{fig:burst}{0.3}{0.3}{0.3}


When the burstiness parameter is at its maximum of 1, the packets arrive to the Buffers exactly when a frame is generated. For burstiness parameters smaller than 1, the overlap parameter may also play a role. See, it's possible that packets get so spread out so much they start to cross over with packets from other frames. Figure \ref{fig:overlap} illustrates the difference. The minimum value the burstiness parameter can take is 0, leading to a constant bitrate, the bitrate defined at the start.


\imagesidebysidecomplete{Implementation/AppLayer/overlap1.png}{overlap off}{fig:overlap1}{.5}{Implementation/AppLayer/overlap2.png}{overlap on}{fig:overlap2}{.5}{big caption}{fig:overlap}{0.5}{0.5}


All the graphs happen for the downlink. Depending on a VR or AR room, there are different amounts of traffic in the DL. If it's VR, then all users need to be received in the \acs{HMD}, including the users physically present in the room. If AR is chosen, then the physical users can see each other, as such the information on the downlink is proportional to the virtual users only.

Much like the format of saving and transmitting 3D representations, it still is uncertain how to orchestrate this XR meeting. Recording 2 streams from the same user will result in an uplink that is half of each stream? Because there is an overlap when recording. We consider that there's no gain in stitching, i.e. the information recorded in each camera is simply aggregated/summed. 


The figures above were generates for a meeting with 2 PHY and 2 VIR users. By counting the packets in figure \ref{fig:packet1}, one sees 3200 in the first frame and 640 in the remaining. Note that 640 is 20\% of 3200, which is our IP frame ratio. A total of 6400 packets of 1500 bytes each for 200 ms is equivalent to a throughput of $4 \times 96$ Mbit/s, and 96 Mbit/s was the average bitrate defined for each UL stream.



There's a final modelling choice to take into account. In the meeting, we need to coordinate the arrivals of the packets for the different users, are the I frames going to arrive at the same time or close to that, or is there an effort on the uplink devices to space I frames from different devices as uniformly as possible in time such that it's more likely that the network is not overloaded? For the uplink the latter, for the downlink the first. We've assumed the uplink camera hubs should be able to coordinate the start of the transmission and stick with it. Another good way of modelling would be to do it randomly, but several runs would be required. Concerning the downlink, the choice was to have the I frames all come at the same time because they all come from the MCU and they are not user specific. Being user specific here means a \ii{cloud rendered} approach where the \acs{MCU} had to render all information \ii{a priori} for each individual user, which would require much more resources on the could and would incur in considerably higher latencies. Instead, all the information all users in the room require is sent to the BS and the BS then distributes to the users the information they specifically require, which can be the complete meeting anyway, but it can also be information on only a set of participants. 






In terms of implementation, the phases are well defined as well. It starts with a Frame Sequence, then packets are generated per frame and an Packet Sequence is created. And from a Packet Sequence, a Buffer is created and packets from the Packet Sequence are added to the buffer. 

Social-XR meetings of the future are expected to require throughputs on the order of 100 Mbit/s \cite{3gpp.26.928}. This represents more than 8000 packets arriving every second. Tracking all packets, for every user, represents a considerable memory burden. Instead, a memory-efficient system is in place. And that system is the update of buffers with packets. A \acs{GoP} of 6 frames, at 30 \acs{FPS} takes 200 ms. After that period, it is repeated, and the packets are added once again to the buffer, but with a different absolute time of arrival.


The network considerations for the application traffic model were based specially in \cite{multi-sensor-sylvie, vr-simon} (Sylvie and Simon, TNO).

